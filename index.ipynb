{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Tuning Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this lab on initialization and optimization, let's look at a slightly different type of neural network. This time, we will not perform a classification task as we've done before (Santa vs not santa, bank complaint types), but we'll look at a linear regression problem.\n",
    "\n",
    "We can just as well use deep learning networks for linear regression as for a classification problem. Do note that getting regression to work with neural networks is a hard problem because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Build a nueral network using keras\n",
    "* Normalize your data to assist algorithm convergence\n",
    "* Implement and observe the impact of various initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to facebook posts published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set and delete any rows with missing data. Afterwards, briefly preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load the dataset and drop rows with missing values. Then preview the data.\n",
    "df = pd.read_csv('dataset_Facebook.csv', sep=';', header=0)\n",
    "df = df.dropna()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our input data. We'll use the 7 first columns as our predictors. We'll do the following two things:\n",
    "- Normalize the continuous variables --> you can do this using `np.mean()` and `np.std()`\n",
    "- Make dummy variables of the categorical variables (you can do this by using `pd.get_dummies`)\n",
    "\n",
    "We only count \"Category\" and \"Type\" as categorical variables. Note that you can argue that \"Post month\", \"Post Weekday\" and \"Post Hour\" can also be considered categories, but we'll just treat them as being continuous for now.\n",
    "\n",
    "You'll then use these to define X and Y. \n",
    "\n",
    "To summarize, X will be:\n",
    "* Page total likes\n",
    "* Post Month\n",
    "* Post Weekday\n",
    "* Post Hour\n",
    "* Paid\n",
    "along with dummy variables for:\n",
    "* Type\n",
    "* Category\n",
    "\n",
    "\n",
    "Be sure to normalize your features by subtracting the mean and dividing by the standard deviation.  \n",
    "\n",
    "Finally, y will simply be the \"like\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  9,  5, 12, 11,  2,  4, 13,  8,  7,  6,  1, 14, 23, 22, 15,\n",
       "       20, 19, 18, 17, 16], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Post Hour\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Page total likes', 'Type', 'Category', 'Post Month', 'Post Weekday',\n",
       "       'Post Hour', 'Paid', 'Lifetime Post Total Reach',\n",
       "       'Lifetime Post Total Impressions', 'Lifetime Engaged Users',\n",
       "       'Lifetime Post Consumers', 'Lifetime Post Consumptions',\n",
       "       'Lifetime Post Impressions by people who have liked your Page',\n",
       "       'Lifetime Post reach by people who like your Page',\n",
       "       'Lifetime People who have liked your Page and engaged with your post',\n",
       "       'comment', 'like', 'share', 'Total Interactions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data[\"Page total likes\"]\n",
    "X1 = data[\"Type\"]\n",
    "X2 = data[\"Category\"]\n",
    "X3 = data[\"Post Month\"]\n",
    "X4 = data[\"Post Weekday\"]\n",
    "X5 = data[\"Post Hour\"]\n",
    "X6 = data[\"Paid\"]\n",
    "\n",
    "## standardize/categorize\n",
    "X0= (X0-np.mean(X0))/(np.std(X0))\n",
    "dummy_X1= pd.get_dummies(X1)\n",
    "dummy_X2= pd.get_dummies(X2)\n",
    "X3= (X3-np.mean(X3))/(np.std(X3))\n",
    "X4= (X4-np.mean(X4))/(np.std(X4))\n",
    "X5= (X5-np.mean(X5))/(np.std(X5))\n",
    "\n",
    "X = pd.concat([X0, dummy_X1, dummy_X2, X3, X4, X5, X6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Status</th>\n",
       "      <th>Video</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes  Link  Photo  Status  Video  1  2  3  Post Month  \\\n",
       "0           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "1           1.00496     0      0       1      0  0  1  0    1.506154   \n",
       "2           1.00496     0      1       0      0  0  0  1    1.506154   \n",
       "3           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "4           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "\n",
       "   Post Weekday  Post Hour  Paid  \n",
       "0     -0.065724  -1.105878   0.0  \n",
       "1     -0.558655   0.492065   0.0  \n",
       "2     -0.558655  -1.105878   0.0  \n",
       "3     -1.051585   0.492065   1.0  \n",
       "4     -1.051585  -1.105878   0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      79.0\n",
       "1     130.0\n",
       "2      66.0\n",
       "3    1572.0\n",
       "4     325.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data[\"like\"]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fairly small. Let's just split the data up in a training set and a validation set!  The next three code blocks are all provided for you; have a quick review but not need to make edits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided; defining training and validation sets\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Status</th>\n",
       "      <th>Video</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.557700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.294376</td>\n",
       "      <td>1.413068</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.150656</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.311513</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.192768</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.183335</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.311513</td>\n",
       "      <td>0.427207</td>\n",
       "      <td>0.720342</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.004960</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>-0.943093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.917402</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>1.176897</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Page total likes  Link  Photo  Status  Video  1  2  3  Post Month  \\\n",
       "208          0.557700     0      1       0      0  0  0  1    0.294376   \n",
       "290          0.150656     0      1       0      0  1  0  0   -0.311513   \n",
       "286          0.183335     0      1       0      0  1  0  0   -0.311513   \n",
       "0            1.004960     0      1       0      0  0  1  0    1.506154   \n",
       "401         -0.943093     0      1       0      0  1  0  0   -0.917402   \n",
       "\n",
       "     Post Weekday  Post Hour  Paid  \n",
       "208      1.413068  -1.105878   0.0  \n",
       "290     -0.065724  -0.192768   0.0  \n",
       "286      0.427207   0.720342   1.0  \n",
       "0       -0.065724  -1.105878   0.0  \n",
       "401     -0.558655   1.176897   1.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 463us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 69us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 54us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 50/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    }
   ],
   "source": [
    "#Code provided; building an initial model\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code provided; previewing the loss through successive epochs\n",
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see what happend? all the values for training and validation loss are \"nan\". There could be several reasons for that, but as we already mentioned there is likely a vanishing or exploding gradient problem. recall that we normalized out inputs. But how about the outputs? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208     54.0\n",
       "290     23.0\n",
       "286     15.0\n",
       "0       79.0\n",
       "401    329.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, indeed. We didn't normalize them and we should, as they take pretty high values. Let\n",
    "s rerun the model but make sure that the output is normalized as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the output\n",
    "\n",
    "Normalize Y as you did X by subtracting the mean and dividing by the standard deviation. Then, resplit the data into training and validation sets as we demonstrated above, and retrain a new model using your normalized X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here: redefine Y after normalizing the data.\n",
    "Y = (data['like'] - np.mean(data['like'])) / np.std(data['like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.309011\n",
       "1   -0.151644\n",
       "2   -0.349124\n",
       "3    4.297815\n",
       "4    0.450051\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.950000e+02\n",
       "mean    -1.794300e-17\n",
       "std      1.001012e+00\n",
       "min     -5.527742e-01\n",
       "25%     -3.768941e-01\n",
       "50%     -2.411269e-01\n",
       "75%      2.732173e-02\n",
       "max      1.540604e+01\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; create training and validation sets as before. Use random seed 123.\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Status</th>\n",
       "      <th>Video</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.309011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.151644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.349124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.297815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes  Link  Photo  Status  Video  1  2  3  Post Month  \\\n",
       "0           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "1           1.00496     0      0       1      0  0  1  0    1.506154   \n",
       "2           1.00496     0      1       0      0  0  0  1    1.506154   \n",
       "3           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "4           1.00496     0      1       0      0  0  1  0    1.506154   \n",
       "\n",
       "   Post Weekday  Post Hour  Paid      like  \n",
       "0     -0.065724  -1.105878   0.0 -0.309011  \n",
       "1     -0.558655   0.492065   0.0 -0.151644  \n",
       "2     -0.558655  -1.105878   0.0 -0.349124  \n",
       "3     -1.051585   0.492065   1.0  4.297815  \n",
       "4     -1.051585  -1.105878   0.0  0.450051  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 690us/step - loss: 1.3721 - mean_squared_error: 1.3721 - val_loss: 1.1379 - val_mean_squared_error: 1.1379\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.2270 - mean_squared_error: 1.2270 - val_loss: 1.0890 - val_mean_squared_error: 1.0890\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.1753 - mean_squared_error: 1.1753 - val_loss: 1.0659 - val_mean_squared_error: 1.0659\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1450 - mean_squared_error: 1.1450 - val_loss: 1.0485 - val_mean_squared_error: 1.0485\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1234 - mean_squared_error: 1.1234 - val_loss: 1.0366 - val_mean_squared_error: 1.0366\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1054 - mean_squared_error: 1.1054 - val_loss: 1.0287 - val_mean_squared_error: 1.0287\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0896 - mean_squared_error: 1.0896 - val_loss: 1.0181 - val_mean_squared_error: 1.0181\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0777 - mean_squared_error: 1.0777 - val_loss: 1.0141 - val_mean_squared_error: 1.0141\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0679 - mean_squared_error: 1.0679 - val_loss: 1.0081 - val_mean_squared_error: 1.0081\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0581 - mean_squared_error: 1.0581 - val_loss: 1.0029 - val_mean_squared_error: 1.0029\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0498 - mean_squared_error: 1.0498 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0438 - mean_squared_error: 1.0438 - val_loss: 0.9947 - val_mean_squared_error: 0.9947\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0384 - mean_squared_error: 1.0384 - val_loss: 0.9910 - val_mean_squared_error: 0.9910\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0343 - mean_squared_error: 1.0343 - val_loss: 0.9794 - val_mean_squared_error: 0.9794\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0295 - mean_squared_error: 1.0295 - val_loss: 0.9801 - val_mean_squared_error: 0.9801\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0247 - mean_squared_error: 1.0247 - val_loss: 0.9818 - val_mean_squared_error: 0.9818\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 0.9811 - val_mean_squared_error: 0.9811\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 0.9810 - val_mean_squared_error: 0.9810\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 0.9821 - val_mean_squared_error: 0.9821\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0139 - mean_squared_error: 1.0139 - val_loss: 0.9799 - val_mean_squared_error: 0.9799\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0112 - mean_squared_error: 1.0112 - val_loss: 0.9809 - val_mean_squared_error: 0.9809\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0101 - mean_squared_error: 1.0101 - val_loss: 0.9783 - val_mean_squared_error: 0.9783\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 0.9748 - val_mean_squared_error: 0.9748\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0060 - mean_squared_error: 1.0060 - val_loss: 0.9745 - val_mean_squared_error: 0.9745\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0040 - mean_squared_error: 1.0040 - val_loss: 0.9751 - val_mean_squared_error: 0.9751\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0027 - mean_squared_error: 1.0027 - val_loss: 0.9766 - val_mean_squared_error: 0.9766\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0011 - mean_squared_error: 1.0011 - val_loss: 0.9737 - val_mean_squared_error: 0.9737\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0003 - mean_squared_error: 1.0003 - val_loss: 0.9736 - val_mean_squared_error: 0.9736\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9981 - mean_squared_error: 0.9981 - val_loss: 0.9744 - val_mean_squared_error: 0.9744\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9975 - mean_squared_error: 0.9975 - val_loss: 0.9739 - val_mean_squared_error: 0.9739\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9949 - mean_squared_error: 0.9949 - val_loss: 0.9757 - val_mean_squared_error: 0.9757\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9944 - mean_squared_error: 0.9944 - val_loss: 0.9758 - val_mean_squared_error: 0.9758\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9934 - mean_squared_error: 0.9934 - val_loss: 0.9738 - val_mean_squared_error: 0.9738\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9915 - mean_squared_error: 0.9915 - val_loss: 0.9714 - val_mean_squared_error: 0.9714\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9918 - mean_squared_error: 0.9918 - val_loss: 0.9721 - val_mean_squared_error: 0.9721\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.1446 - mean_squared_error: 0.14 - 0s 76us/step - loss: 0.9903 - mean_squared_error: 0.9903 - val_loss: 0.9732 - val_mean_squared_error: 0.9732\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9902 - mean_squared_error: 0.9902 - val_loss: 0.9702 - val_mean_squared_error: 0.9702\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9897 - mean_squared_error: 0.9897 - val_loss: 0.9717 - val_mean_squared_error: 0.9717\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9891 - mean_squared_error: 0.9891 - val_loss: 0.9702 - val_mean_squared_error: 0.9702\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9885 - mean_squared_error: 0.9885 - val_loss: 0.9681 - val_mean_squared_error: 0.9681\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9900 - mean_squared_error: 0.9900 - val_loss: 0.9681 - val_mean_squared_error: 0.9681\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 0.9880 - mean_squared_error: 0.9880 - val_loss: 0.9669 - val_mean_squared_error: 0.9669\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9870 - mean_squared_error: 0.9870 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 0.9661 - val_mean_squared_error: 0.9661\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9877 - mean_squared_error: 0.9877 - val_loss: 0.9659 - val_mean_squared_error: 0.9659\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9856 - mean_squared_error: 0.9856 - val_loss: 0.9657 - val_mean_squared_error: 0.9657\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9856 - mean_squared_error: 0.9856 - val_loss: 0.9651 - val_mean_squared_error: 0.9651\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9854 - mean_squared_error: 0.9854 - val_loss: 0.9682 - val_mean_squared_error: 0.9682\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9858 - mean_squared_error: 0.9858 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 78us/step - loss: 0.9844 - mean_squared_error: 0.9844 - val_loss: 0.9640 - val_mean_squared_error: 0.9640\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9844 - mean_squared_error: 0.9844 - val_loss: 0.9637 - val_mean_squared_error: 0.9637\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9840 - mean_squared_error: 0.9840 - val_loss: 0.9622 - val_mean_squared_error: 0.9622\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9830 - mean_squared_error: 0.9830 - val_loss: 0.9631 - val_mean_squared_error: 0.9631\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9831 - mean_squared_error: 0.9831 - val_loss: 0.9527 - val_mean_squared_error: 0.9527\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9844 - mean_squared_error: 0.9844 - val_loss: 0.9518 - val_mean_squared_error: 0.9518\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 0.9855 - mean_squared_error: 0.9855 - val_loss: 0.9559 - val_mean_squared_error: 0.9559\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9833 - mean_squared_error: 0.9833 - val_loss: 0.9586 - val_mean_squared_error: 0.9586\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9818 - mean_squared_error: 0.9818 - val_loss: 0.9610 - val_mean_squared_error: 0.9610\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9817 - mean_squared_error: 0.9817 - val_loss: 0.9582 - val_mean_squared_error: 0.9582\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9804 - mean_squared_error: 0.9804 - val_loss: 0.9617 - val_mean_squared_error: 0.9617\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9816 - mean_squared_error: 0.9816 - val_loss: 0.9587 - val_mean_squared_error: 0.9587\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9803 - mean_squared_error: 0.9803 - val_loss: 0.9624 - val_mean_squared_error: 0.9624\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 0.9587 - val_mean_squared_error: 0.9587\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9802 - mean_squared_error: 0.9802 - val_loss: 0.9561 - val_mean_squared_error: 0.9561\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9791 - mean_squared_error: 0.9791 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9794 - mean_squared_error: 0.9794 - val_loss: 0.9564 - val_mean_squared_error: 0.9564\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9791 - mean_squared_error: 0.9791 - val_loss: 0.9585 - val_mean_squared_error: 0.9585\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9793 - mean_squared_error: 0.9793 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9798 - mean_squared_error: 0.9798 - val_loss: 0.9580 - val_mean_squared_error: 0.9580\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9782 - mean_squared_error: 0.9782 - val_loss: 0.9592 - val_mean_squared_error: 0.9592\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9780 - mean_squared_error: 0.9780 - val_loss: 0.9569 - val_mean_squared_error: 0.9569\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9778 - mean_squared_error: 0.9778 - val_loss: 0.9582 - val_mean_squared_error: 0.9582\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9775 - mean_squared_error: 0.9775 - val_loss: 0.9568 - val_mean_squared_error: 0.9568\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9773 - mean_squared_error: 0.9773 - val_loss: 0.9546 - val_mean_squared_error: 0.9546\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9777 - mean_squared_error: 0.9777 - val_loss: 0.9571 - val_mean_squared_error: 0.9571\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9777 - mean_squared_error: 0.9777 - val_loss: 0.9573 - val_mean_squared_error: 0.9573\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9770 - mean_squared_error: 0.9770 - val_loss: 0.9604 - val_mean_squared_error: 0.9604\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9790 - mean_squared_error: 0.9790 - val_loss: 0.9531 - val_mean_squared_error: 0.9531\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9762 - mean_squared_error: 0.9762 - val_loss: 0.9501 - val_mean_squared_error: 0.9501\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9757 - mean_squared_error: 0.9757 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9762 - mean_squared_error: 0.9762 - val_loss: 0.9526 - val_mean_squared_error: 0.9526\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9760 - mean_squared_error: 0.9760 - val_loss: 0.9528 - val_mean_squared_error: 0.9528\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9759 - mean_squared_error: 0.9759 - val_loss: 0.9534 - val_mean_squared_error: 0.9534\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9754 - mean_squared_error: 0.9754 - val_loss: 0.9540 - val_mean_squared_error: 0.9540\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9753 - mean_squared_error: 0.9753 - val_loss: 0.9549 - val_mean_squared_error: 0.9549\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9753 - mean_squared_error: 0.9753 - val_loss: 0.9516 - val_mean_squared_error: 0.9516\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9753 - mean_squared_error: 0.9753 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9742 - mean_squared_error: 0.9742 - val_loss: 0.9482 - val_mean_squared_error: 0.9482\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9746 - mean_squared_error: 0.9746 - val_loss: 0.9427 - val_mean_squared_error: 0.9427\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9759 - mean_squared_error: 0.9759 - val_loss: 0.9455 - val_mean_squared_error: 0.9455\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9750 - mean_squared_error: 0.9750 - val_loss: 0.9466 - val_mean_squared_error: 0.9466\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9742 - mean_squared_error: 0.9742 - val_loss: 0.9488 - val_mean_squared_error: 0.9488\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9731 - mean_squared_error: 0.9731 - val_loss: 0.9511 - val_mean_squared_error: 0.9511\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 0.9542 - val_mean_squared_error: 0.9542\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9743 - mean_squared_error: 0.9743 - val_loss: 0.9554 - val_mean_squared_error: 0.9554\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9737 - mean_squared_error: 0.9737 - val_loss: 0.9522 - val_mean_squared_error: 0.9522\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9736 - mean_squared_error: 0.9736 - val_loss: 0.9527 - val_mean_squared_error: 0.9527\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 0.9514 - val_mean_squared_error: 0.9514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9727 - mean_squared_error: 0.9727 - val_loss: 0.9529 - val_mean_squared_error: 0.9529\n"
     ]
    }
   ],
   "source": [
    "#Your code here; rebuild a simple model using a relu layer followed by a linear layer. (See our code snippet above!)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's recheck our loss function. Not only should it be populated with numerical data as opposed to null values, but we also should expect to see the loss function decreasing with successive epochs, demonstrating optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3720854445539339,\n",
       " 1.2270435284484515,\n",
       " 1.1753324318413783,\n",
       " 1.1450198280690895,\n",
       " 1.123367585317053,\n",
       " 1.1054006512417938,\n",
       " 1.089552349815465,\n",
       " 1.0777066149043315,\n",
       " 1.0678775272887162,\n",
       " 1.058149522753677]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a converged model. With that, let's investigate how well the model performed with our good old friend, mean squarred error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_train: 0.9707142903228307\n",
      "MSE_val: 0.952940722303891\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)  \n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "\n",
    "print(\"MSE_train:\", MSE_train)\n",
    "print(\"MSE_val:\", MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weight Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and use a weight initializer. In the lecture, we've seen the He normalizer, which initializes the weight vector to have an average 0 and a variance of 2/n, with $n$ the number of features feeding into a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 907us/step - loss: 1.5102 - mean_squared_error: 1.5102 - val_loss: 1.2863 - val_mean_squared_error: 1.2863\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.2107 - mean_squared_error: 1.2107 - val_loss: 1.1489 - val_mean_squared_error: 1.1489\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 1.1401 - mean_squared_error: 1.1401 - val_loss: 1.0889 - val_mean_squared_error: 1.0889\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1068 - mean_squared_error: 1.1068 - val_loss: 1.0574 - val_mean_squared_error: 1.0574\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0850 - mean_squared_error: 1.0850 - val_loss: 1.0377 - val_mean_squared_error: 1.0377\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0678 - mean_squared_error: 1.0678 - val_loss: 1.0239 - val_mean_squared_error: 1.0239\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0554 - mean_squared_error: 1.0554 - val_loss: 1.0121 - val_mean_squared_error: 1.0121\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0443 - mean_squared_error: 1.0443 - val_loss: 1.0033 - val_mean_squared_error: 1.0033\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0359 - mean_squared_error: 1.0359 - val_loss: 0.9973 - val_mean_squared_error: 0.9973\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0291 - mean_squared_error: 1.0291 - val_loss: 0.9937 - val_mean_squared_error: 0.9937\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 0.9868 - val_mean_squared_error: 0.9868\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 0.9835 - val_mean_squared_error: 0.9835\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 0.9773 - val_mean_squared_error: 0.9773\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.9699 - val_mean_squared_error: 0.9699\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 0.9671 - val_mean_squared_error: 0.9671\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0011 - mean_squared_error: 1.0011 - val_loss: 0.9642 - val_mean_squared_error: 0.9642\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9976 - mean_squared_error: 0.9976 - val_loss: 0.9609 - val_mean_squared_error: 0.9609\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9949 - mean_squared_error: 0.9949 - val_loss: 0.9599 - val_mean_squared_error: 0.9599\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9921 - mean_squared_error: 0.9921 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9906 - mean_squared_error: 0.9906 - val_loss: 0.9548 - val_mean_squared_error: 0.9548\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9883 - mean_squared_error: 0.9883 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9865 - mean_squared_error: 0.9865 - val_loss: 0.9452 - val_mean_squared_error: 0.9452\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9845 - mean_squared_error: 0.9845 - val_loss: 0.9456 - val_mean_squared_error: 0.9456\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9822 - mean_squared_error: 0.9822 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9811 - mean_squared_error: 0.9811 - val_loss: 0.9478 - val_mean_squared_error: 0.9478\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 0.9799 - mean_squared_error: 0.9799 - val_loss: 0.9484 - val_mean_squared_error: 0.9484\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9776 - mean_squared_error: 0.9776 - val_loss: 0.9453 - val_mean_squared_error: 0.9453\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9754 - mean_squared_error: 0.9754 - val_loss: 0.9448 - val_mean_squared_error: 0.9448\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.1945 - mean_squared_error: 0.19 - 0s 58us/step - loss: 0.9750 - mean_squared_error: 0.9750 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9741 - mean_squared_error: 0.9741 - val_loss: 0.9392 - val_mean_squared_error: 0.9392\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 0.9434 - val_mean_squared_error: 0.9434\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9707 - mean_squared_error: 0.9707 - val_loss: 0.9438 - val_mean_squared_error: 0.9438\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9701 - mean_squared_error: 0.9701 - val_loss: 0.9445 - val_mean_squared_error: 0.9445\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9688 - mean_squared_error: 0.9688 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9678 - mean_squared_error: 0.9678 - val_loss: 0.9435 - val_mean_squared_error: 0.9435\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 0.9661 - mean_squared_error: 0.9661 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9656 - mean_squared_error: 0.9656 - val_loss: 0.9422 - val_mean_squared_error: 0.9422\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9642 - mean_squared_error: 0.9642 - val_loss: 0.9385 - val_mean_squared_error: 0.9385\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9637 - mean_squared_error: 0.9637 - val_loss: 0.9387 - val_mean_squared_error: 0.9387\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9624 - mean_squared_error: 0.9624 - val_loss: 0.9356 - val_mean_squared_error: 0.9356\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9616 - mean_squared_error: 0.9616 - val_loss: 0.9378 - val_mean_squared_error: 0.9378\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9608 - mean_squared_error: 0.9608 - val_loss: 0.9362 - val_mean_squared_error: 0.9362\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9599 - mean_squared_error: 0.9599 - val_loss: 0.9387 - val_mean_squared_error: 0.9387\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9604 - mean_squared_error: 0.9604 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9582 - mean_squared_error: 0.9582 - val_loss: 0.9399 - val_mean_squared_error: 0.9399\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 0.9359 - val_mean_squared_error: 0.9359\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9569 - mean_squared_error: 0.9569 - val_loss: 0.9373 - val_mean_squared_error: 0.9373\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 0.9378 - val_mean_squared_error: 0.9378\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9554 - mean_squared_error: 0.9554 - val_loss: 0.9406 - val_mean_squared_error: 0.9406\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 71us/step - loss: 0.9552 - mean_squared_error: 0.9552 - val_loss: 0.9396 - val_mean_squared_error: 0.9396\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9541 - mean_squared_error: 0.9541 - val_loss: 0.9394 - val_mean_squared_error: 0.9394\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9529 - mean_squared_error: 0.9529 - val_loss: 0.9366 - val_mean_squared_error: 0.9366\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9530 - mean_squared_error: 0.9530 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9526 - mean_squared_error: 0.9526 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9512 - mean_squared_error: 0.9512 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9511 - mean_squared_error: 0.9511 - val_loss: 0.9394 - val_mean_squared_error: 0.9394\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9502 - mean_squared_error: 0.9502 - val_loss: 0.9417 - val_mean_squared_error: 0.9417\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9502 - mean_squared_error: 0.9502 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9515 - mean_squared_error: 0.9515 - val_loss: 0.9447 - val_mean_squared_error: 0.9447\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9491 - mean_squared_error: 0.9491 - val_loss: 0.9421 - val_mean_squared_error: 0.9421\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9487 - mean_squared_error: 0.9487 - val_loss: 0.9409 - val_mean_squared_error: 0.9409\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9477 - mean_squared_error: 0.9477 - val_loss: 0.9406 - val_mean_squared_error: 0.9406\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9477 - mean_squared_error: 0.9477 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9467 - mean_squared_error: 0.9467 - val_loss: 0.9462 - val_mean_squared_error: 0.9462\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9465 - mean_squared_error: 0.9465 - val_loss: 0.9449 - val_mean_squared_error: 0.9449\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9471 - mean_squared_error: 0.9471 - val_loss: 0.9454 - val_mean_squared_error: 0.9454\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9451 - mean_squared_error: 0.9451 - val_loss: 0.9452 - val_mean_squared_error: 0.9452\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9450 - mean_squared_error: 0.9450 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9445 - mean_squared_error: 0.9445 - val_loss: 0.9461 - val_mean_squared_error: 0.9461\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9443 - mean_squared_error: 0.9443 - val_loss: 0.9454 - val_mean_squared_error: 0.9454\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9434 - mean_squared_error: 0.9434 - val_loss: 0.9447 - val_mean_squared_error: 0.9447\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9421 - mean_squared_error: 0.9421 - val_loss: 0.9395 - val_mean_squared_error: 0.9395\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9424 - mean_squared_error: 0.9424 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9420 - mean_squared_error: 0.9420 - val_loss: 0.9434 - val_mean_squared_error: 0.9434\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9421 - mean_squared_error: 0.9421 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9406 - mean_squared_error: 0.9406 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9405 - mean_squared_error: 0.9405 - val_loss: 0.9448 - val_mean_squared_error: 0.9448\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9398 - mean_squared_error: 0.9398 - val_loss: 0.9450 - val_mean_squared_error: 0.9450\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 72us/step - loss: 0.9394 - mean_squared_error: 0.9394 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9391 - mean_squared_error: 0.9391 - val_loss: 0.9431 - val_mean_squared_error: 0.9431\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9385 - mean_squared_error: 0.9385 - val_loss: 0.9454 - val_mean_squared_error: 0.9454\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9384 - mean_squared_error: 0.9384 - val_loss: 0.9453 - val_mean_squared_error: 0.9453\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9372 - mean_squared_error: 0.9372 - val_loss: 0.9357 - val_mean_squared_error: 0.9357\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9379 - mean_squared_error: 0.9379 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9361 - mean_squared_error: 0.9361 - val_loss: 0.9380 - val_mean_squared_error: 0.9380\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9359 - mean_squared_error: 0.9359 - val_loss: 0.9362 - val_mean_squared_error: 0.9362\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9353 - mean_squared_error: 0.9353 - val_loss: 0.9386 - val_mean_squared_error: 0.9386\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9342 - mean_squared_error: 0.9342 - val_loss: 0.9431 - val_mean_squared_error: 0.9431\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9328 - mean_squared_error: 0.9328 - val_loss: 0.9465 - val_mean_squared_error: 0.9465\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9329 - mean_squared_error: 0.9329 - val_loss: 0.9496 - val_mean_squared_error: 0.9496\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9320 - mean_squared_error: 0.9320 - val_loss: 0.9506 - val_mean_squared_error: 0.9506\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9322 - mean_squared_error: 0.9322 - val_loss: 0.9507 - val_mean_squared_error: 0.9507\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9315 - mean_squared_error: 0.9315 - val_loss: 0.9512 - val_mean_squared_error: 0.9512\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9303 - mean_squared_error: 0.9303 - val_loss: 0.9514 - val_mean_squared_error: 0.9514\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9309 - mean_squared_error: 0.9309 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 1.3194 - mean_squared_error: 1.31 - 0s 68us/step - loss: 0.9306 - mean_squared_error: 0.9306 - val_loss: 0.9468 - val_mean_squared_error: 0.9468\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9304 - mean_squared_error: 0.9304 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 73us/step - loss: 0.9297 - mean_squared_error: 0.9297 - val_loss: 0.9508 - val_mean_squared_error: 0.9508\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9292 - mean_squared_error: 0.9292 - val_loss: 0.9477 - val_mean_squared_error: 0.9477\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, kernel_initializer= \"he_normal\",\n",
    "                activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266121377054887\n",
      "0.947702462558325\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initializer does not really help us to decrease the MSE. We know that initializers can be particularly helpful in deeper networks, and our network isn't very deep. What if we use the `Lecun` initializer with a `tanh` activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 1ms/step - loss: 1.2427 - mean_squared_error: 1.2427 - val_loss: 1.0645 - val_mean_squared_error: 1.0645\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1424 - mean_squared_error: 1.1424 - val_loss: 1.0116 - val_mean_squared_error: 1.0116\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.0868 - mean_squared_error: 1.0868 - val_loss: 0.9843 - val_mean_squared_error: 0.9843\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.0533 - mean_squared_error: 1.0533 - val_loss: 0.9681 - val_mean_squared_error: 0.9681\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0336 - mean_squared_error: 1.0336 - val_loss: 0.9608 - val_mean_squared_error: 0.9608\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.9483 - val_mean_squared_error: 0.9483\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9972 - mean_squared_error: 0.9972 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 0.9461 - val_mean_squared_error: 0.9461\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 0.9492 - val_mean_squared_error: 0.9492\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 0.9464 - val_mean_squared_error: 0.9464\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9786 - mean_squared_error: 0.9786 - val_loss: 0.9476 - val_mean_squared_error: 0.9476\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9757 - mean_squared_error: 0.9757 - val_loss: 0.9426 - val_mean_squared_error: 0.9426\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9742 - mean_squared_error: 0.9742 - val_loss: 0.9389 - val_mean_squared_error: 0.9389\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9707 - mean_squared_error: 0.9707 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9687 - mean_squared_error: 0.9687 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9659 - mean_squared_error: 0.9659 - val_loss: 0.9398 - val_mean_squared_error: 0.9398\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9646 - mean_squared_error: 0.9646 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9626 - mean_squared_error: 0.9626 - val_loss: 0.9448 - val_mean_squared_error: 0.9448\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9632 - mean_squared_error: 0.9632 - val_loss: 0.9385 - val_mean_squared_error: 0.9385\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9613 - mean_squared_error: 0.9613 - val_loss: 0.9342 - val_mean_squared_error: 0.9342\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9610 - mean_squared_error: 0.9610 - val_loss: 0.9300 - val_mean_squared_error: 0.9300\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 0.9352 - val_mean_squared_error: 0.9352\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9574 - mean_squared_error: 0.9574 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 0.9497 - val_mean_squared_error: 0.9497\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9561 - mean_squared_error: 0.9561 - val_loss: 0.9465 - val_mean_squared_error: 0.9465\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9539 - mean_squared_error: 0.9539 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9549 - mean_squared_error: 0.9549 - val_loss: 0.9433 - val_mean_squared_error: 0.9433\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9561 - mean_squared_error: 0.9561 - val_loss: 0.9364 - val_mean_squared_error: 0.9364\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9535 - mean_squared_error: 0.9535 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9524 - mean_squared_error: 0.9524 - val_loss: 0.9468 - val_mean_squared_error: 0.9468\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9523 - mean_squared_error: 0.9523 - val_loss: 0.9499 - val_mean_squared_error: 0.9499\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9519 - mean_squared_error: 0.9519 - val_loss: 0.9524 - val_mean_squared_error: 0.9524\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9517 - mean_squared_error: 0.9517 - val_loss: 0.9509 - val_mean_squared_error: 0.9509\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9499 - mean_squared_error: 0.9499 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9497 - mean_squared_error: 0.9497 - val_loss: 0.9526 - val_mean_squared_error: 0.9526\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9496 - mean_squared_error: 0.9496 - val_loss: 0.9451 - val_mean_squared_error: 0.9451\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9491 - mean_squared_error: 0.9491 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 0.9478 - mean_squared_error: 0.9478 - val_loss: 0.9419 - val_mean_squared_error: 0.9419\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 0.9475 - val_mean_squared_error: 0.9475\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 84us/step - loss: 0.9480 - mean_squared_error: 0.9480 - val_loss: 0.9423 - val_mean_squared_error: 0.9423\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 0.9478 - val_mean_squared_error: 0.9478\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9492 - mean_squared_error: 0.9492 - val_loss: 0.9532 - val_mean_squared_error: 0.9532\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9467 - mean_squared_error: 0.9467 - val_loss: 0.9537 - val_mean_squared_error: 0.9537\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9476 - mean_squared_error: 0.9476 - val_loss: 0.9438 - val_mean_squared_error: 0.9438\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 0.9480 - val_mean_squared_error: 0.9480\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9449 - mean_squared_error: 0.9449 - val_loss: 0.9484 - val_mean_squared_error: 0.9484\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9456 - mean_squared_error: 0.9456 - val_loss: 0.9530 - val_mean_squared_error: 0.9530\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 65us/step - loss: 0.9455 - mean_squared_error: 0.9455 - val_loss: 0.9526 - val_mean_squared_error: 0.9526\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9449 - mean_squared_error: 0.9449 - val_loss: 0.9523 - val_mean_squared_error: 0.9523\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9428 - mean_squared_error: 0.9428 - val_loss: 0.9466 - val_mean_squared_error: 0.9466\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9441 - mean_squared_error: 0.9441 - val_loss: 0.9496 - val_mean_squared_error: 0.9496\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9445 - mean_squared_error: 0.9445 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9426 - mean_squared_error: 0.9426 - val_loss: 0.9465 - val_mean_squared_error: 0.9465\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9432 - mean_squared_error: 0.9432 - val_loss: 0.9470 - val_mean_squared_error: 0.9470\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9422 - mean_squared_error: 0.9422 - val_loss: 0.9517 - val_mean_squared_error: 0.9517\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9430 - mean_squared_error: 0.9430 - val_loss: 0.9542 - val_mean_squared_error: 0.9542\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9457 - mean_squared_error: 0.9457 - val_loss: 0.9566 - val_mean_squared_error: 0.9566\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9425 - mean_squared_error: 0.9425 - val_loss: 0.9532 - val_mean_squared_error: 0.9532\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9416 - mean_squared_error: 0.9416 - val_loss: 0.9488 - val_mean_squared_error: 0.9488\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9411 - mean_squared_error: 0.9411 - val_loss: 0.9476 - val_mean_squared_error: 0.9476\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9412 - mean_squared_error: 0.9412 - val_loss: 0.9536 - val_mean_squared_error: 0.9536\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9412 - mean_squared_error: 0.9412 - val_loss: 0.9562 - val_mean_squared_error: 0.9562\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9413 - mean_squared_error: 0.9413 - val_loss: 0.9538 - val_mean_squared_error: 0.9538\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9435 - mean_squared_error: 0.9435 - val_loss: 0.9526 - val_mean_squared_error: 0.9526\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9398 - mean_squared_error: 0.9398 - val_loss: 0.9507 - val_mean_squared_error: 0.9507\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9411 - mean_squared_error: 0.9411 - val_loss: 0.9544 - val_mean_squared_error: 0.9544\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9405 - mean_squared_error: 0.9405 - val_loss: 0.9527 - val_mean_squared_error: 0.9527\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9398 - mean_squared_error: 0.9398 - val_loss: 0.9497 - val_mean_squared_error: 0.9497\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9397 - mean_squared_error: 0.9397 - val_loss: 0.9473 - val_mean_squared_error: 0.9473\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 0.9368 - val_mean_squared_error: 0.9368\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9388 - mean_squared_error: 0.9388 - val_loss: 0.9470 - val_mean_squared_error: 0.9470\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9385 - mean_squared_error: 0.9385 - val_loss: 0.9486 - val_mean_squared_error: 0.9486\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9392 - mean_squared_error: 0.9392 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9368 - mean_squared_error: 0.9368 - val_loss: 0.9489 - val_mean_squared_error: 0.9489\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9367 - mean_squared_error: 0.9367 - val_loss: 0.9503 - val_mean_squared_error: 0.9503\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9365 - mean_squared_error: 0.9365 - val_loss: 0.9504 - val_mean_squared_error: 0.9504\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9365 - mean_squared_error: 0.9365 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9365 - mean_squared_error: 0.9365 - val_loss: 0.9490 - val_mean_squared_error: 0.9490\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9363 - mean_squared_error: 0.9363 - val_loss: 0.9477 - val_mean_squared_error: 0.9477\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9370 - mean_squared_error: 0.9370 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9355 - mean_squared_error: 0.9355 - val_loss: 0.9296 - val_mean_squared_error: 0.9296\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9394 - mean_squared_error: 0.9394 - val_loss: 0.9360 - val_mean_squared_error: 0.9360\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9355 - mean_squared_error: 0.9355 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9359 - mean_squared_error: 0.9359 - val_loss: 0.9347 - val_mean_squared_error: 0.9347\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 0.9416 - val_mean_squared_error: 0.9416\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9353 - mean_squared_error: 0.9353 - val_loss: 0.9484 - val_mean_squared_error: 0.9484\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9333 - mean_squared_error: 0.9333 - val_loss: 0.9539 - val_mean_squared_error: 0.9539\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9340 - mean_squared_error: 0.9340 - val_loss: 0.9595 - val_mean_squared_error: 0.9595\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9331 - mean_squared_error: 0.9331 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9347 - mean_squared_error: 0.9347 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9331 - mean_squared_error: 0.9331 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9318 - mean_squared_error: 0.9318 - val_loss: 0.9568 - val_mean_squared_error: 0.9568\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9325 - mean_squared_error: 0.9325 - val_loss: 0.9541 - val_mean_squared_error: 0.9541\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9327 - mean_squared_error: 0.9327 - val_loss: 0.9514 - val_mean_squared_error: 0.9514\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9324 - mean_squared_error: 0.9324 - val_loss: 0.9548 - val_mean_squared_error: 0.9548\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9326 - mean_squared_error: 0.9326 - val_loss: 0.9521 - val_mean_squared_error: 0.9521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9321 - mean_squared_error: 0.9321 - val_loss: 0.9463 - val_mean_squared_error: 0.9463\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, \n",
    "                kernel_initializer= \"lecun_normal\", activation='tanh'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9274741851539924\n",
      "0.9463006320907383\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impace of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 1ms/step - loss: 1.2354 - mean_squared_error: 1.2354 - val_loss: 1.3119 - val_mean_squared_error: 1.3119\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1875 - mean_squared_error: 1.1875 - val_loss: 1.2606 - val_mean_squared_error: 1.2606\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.1563 - mean_squared_error: 1.1563 - val_loss: 1.2201 - val_mean_squared_error: 1.2201\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.1315 - mean_squared_error: 1.1315 - val_loss: 1.1852 - val_mean_squared_error: 1.1852\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 1.1585 - val_mean_squared_error: 1.1585\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0940 - mean_squared_error: 1.0940 - val_loss: 1.1334 - val_mean_squared_error: 1.1334\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 1.1130 - val_mean_squared_error: 1.1130\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0664 - mean_squared_error: 1.0664 - val_loss: 1.0933 - val_mean_squared_error: 1.0933\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0546 - mean_squared_error: 1.0546 - val_loss: 1.0782 - val_mean_squared_error: 1.0782\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 1.0448 - mean_squared_error: 1.0448 - val_loss: 1.0653 - val_mean_squared_error: 1.0653\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0365 - mean_squared_error: 1.0365 - val_loss: 1.0540 - val_mean_squared_error: 1.0540\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0279 - mean_squared_error: 1.0279 - val_loss: 1.0434 - val_mean_squared_error: 1.0434\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.0336 - val_mean_squared_error: 1.0336\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.0235 - val_mean_squared_error: 1.0235\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0095 - mean_squared_error: 1.0095 - val_loss: 1.0164 - val_mean_squared_error: 1.0164\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0042 - mean_squared_error: 1.0042 - val_loss: 1.0091 - val_mean_squared_error: 1.0091\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9992 - mean_squared_error: 0.9992 - val_loss: 1.0017 - val_mean_squared_error: 1.0017\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9947 - mean_squared_error: 0.9947 - val_loss: 0.9969 - val_mean_squared_error: 0.9969\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 0.9914 - val_mean_squared_error: 0.9914\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 0.9874 - val_mean_squared_error: 0.9874\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9837 - mean_squared_error: 0.9837 - val_loss: 0.9819 - val_mean_squared_error: 0.9819\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9799 - mean_squared_error: 0.9799 - val_loss: 0.9772 - val_mean_squared_error: 0.9772\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9760 - mean_squared_error: 0.9760 - val_loss: 0.9727 - val_mean_squared_error: 0.9727\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9729 - mean_squared_error: 0.9729 - val_loss: 0.9714 - val_mean_squared_error: 0.9714\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9702 - mean_squared_error: 0.9702 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9660 - mean_squared_error: 0.9660 - val_loss: 0.9653 - val_mean_squared_error: 0.9653\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9631 - mean_squared_error: 0.9631 - val_loss: 0.9619 - val_mean_squared_error: 0.9619\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9611 - mean_squared_error: 0.9611 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 0.9584 - val_mean_squared_error: 0.9584\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9571 - mean_squared_error: 0.9571 - val_loss: 0.9593 - val_mean_squared_error: 0.9593\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9558 - mean_squared_error: 0.9558 - val_loss: 0.9583 - val_mean_squared_error: 0.9583\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9547 - mean_squared_error: 0.9547 - val_loss: 0.9572 - val_mean_squared_error: 0.9572\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9532 - mean_squared_error: 0.9532 - val_loss: 0.9575 - val_mean_squared_error: 0.9575\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9518 - mean_squared_error: 0.9518 - val_loss: 0.9567 - val_mean_squared_error: 0.9567\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9503 - mean_squared_error: 0.9503 - val_loss: 0.9536 - val_mean_squared_error: 0.9536\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9490 - mean_squared_error: 0.9490 - val_loss: 0.9535 - val_mean_squared_error: 0.9535\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9481 - mean_squared_error: 0.9481 - val_loss: 0.9523 - val_mean_squared_error: 0.9523\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9468 - mean_squared_error: 0.9468 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9448 - mean_squared_error: 0.9448 - val_loss: 0.9495 - val_mean_squared_error: 0.9495\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 77us/step - loss: 0.9437 - mean_squared_error: 0.9437 - val_loss: 0.9508 - val_mean_squared_error: 0.9508\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9431 - mean_squared_error: 0.9431 - val_loss: 0.9500 - val_mean_squared_error: 0.9500\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9417 - mean_squared_error: 0.9417 - val_loss: 0.9519 - val_mean_squared_error: 0.9519\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9406 - mean_squared_error: 0.9406 - val_loss: 0.9538 - val_mean_squared_error: 0.9538\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9396 - mean_squared_error: 0.9396 - val_loss: 0.9553 - val_mean_squared_error: 0.9553\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9400 - mean_squared_error: 0.9400 - val_loss: 0.9514 - val_mean_squared_error: 0.9514\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 0.9501 - val_mean_squared_error: 0.9501\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9370 - mean_squared_error: 0.9370 - val_loss: 0.9489 - val_mean_squared_error: 0.9489\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9369 - mean_squared_error: 0.9369 - val_loss: 0.9496 - val_mean_squared_error: 0.9496\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 78us/step - loss: 0.9357 - mean_squared_error: 0.9357 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9357 - mean_squared_error: 0.9357 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9345 - mean_squared_error: 0.9345 - val_loss: 0.9475 - val_mean_squared_error: 0.9475\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 0.9481 - val_mean_squared_error: 0.9481\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9330 - mean_squared_error: 0.9330 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9331 - mean_squared_error: 0.9331 - val_loss: 0.9479 - val_mean_squared_error: 0.9479\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9321 - mean_squared_error: 0.9321 - val_loss: 0.9476 - val_mean_squared_error: 0.9476\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9315 - mean_squared_error: 0.9315 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9318 - mean_squared_error: 0.9318 - val_loss: 0.9510 - val_mean_squared_error: 0.9510\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9316 - mean_squared_error: 0.9316 - val_loss: 0.9494 - val_mean_squared_error: 0.9494\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 0.9306 - mean_squared_error: 0.9306 - val_loss: 0.9483 - val_mean_squared_error: 0.9483\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 101us/step - loss: 0.9306 - mean_squared_error: 0.9306 - val_loss: 0.9475 - val_mean_squared_error: 0.9475\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 101us/step - loss: 0.9301 - mean_squared_error: 0.9301 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9291 - mean_squared_error: 0.9291 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9292 - mean_squared_error: 0.9292 - val_loss: 0.9486 - val_mean_squared_error: 0.9486\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9288 - mean_squared_error: 0.9288 - val_loss: 0.9479 - val_mean_squared_error: 0.9479\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9292 - mean_squared_error: 0.9292 - val_loss: 0.9481 - val_mean_squared_error: 0.9481\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9277 - mean_squared_error: 0.9277 - val_loss: 0.9478 - val_mean_squared_error: 0.9478\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9273 - mean_squared_error: 0.9273 - val_loss: 0.9498 - val_mean_squared_error: 0.9498\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 0.9270 - mean_squared_error: 0.9270 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 0.9457 - val_mean_squared_error: 0.9457\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9268 - mean_squared_error: 0.9268 - val_loss: 0.9455 - val_mean_squared_error: 0.9455\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9257 - mean_squared_error: 0.9257 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9252 - mean_squared_error: 0.9252 - val_loss: 0.9446 - val_mean_squared_error: 0.9446\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9249 - mean_squared_error: 0.9249 - val_loss: 0.9460 - val_mean_squared_error: 0.9460\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9254 - mean_squared_error: 0.9254 - val_loss: 0.9454 - val_mean_squared_error: 0.9454\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9245 - mean_squared_error: 0.9245 - val_loss: 0.9442 - val_mean_squared_error: 0.9442\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9240 - mean_squared_error: 0.9240 - val_loss: 0.9456 - val_mean_squared_error: 0.9456\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9236 - mean_squared_error: 0.9236 - val_loss: 0.9451 - val_mean_squared_error: 0.9451\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9232 - mean_squared_error: 0.9232 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9235 - mean_squared_error: 0.9235 - val_loss: 0.9434 - val_mean_squared_error: 0.9434\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9229 - mean_squared_error: 0.9229 - val_loss: 0.9450 - val_mean_squared_error: 0.9450\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9226 - mean_squared_error: 0.9226 - val_loss: 0.9451 - val_mean_squared_error: 0.9451\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9224 - mean_squared_error: 0.9224 - val_loss: 0.9466 - val_mean_squared_error: 0.9466\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9224 - mean_squared_error: 0.9224 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9211 - mean_squared_error: 0.9211 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9211 - mean_squared_error: 0.9211 - val_loss: 0.9405 - val_mean_squared_error: 0.9405\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9206 - mean_squared_error: 0.9206 - val_loss: 0.9395 - val_mean_squared_error: 0.9395\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9197 - mean_squared_error: 0.9197 - val_loss: 0.9409 - val_mean_squared_error: 0.9409\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9194 - mean_squared_error: 0.9194 - val_loss: 0.9433 - val_mean_squared_error: 0.9433\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9192 - mean_squared_error: 0.9192 - val_loss: 0.9449 - val_mean_squared_error: 0.9449\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9191 - mean_squared_error: 0.9191 - val_loss: 0.9471 - val_mean_squared_error: 0.9471\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9191 - mean_squared_error: 0.9191 - val_loss: 0.9456 - val_mean_squared_error: 0.9456\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9193 - mean_squared_error: 0.9193 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9182 - mean_squared_error: 0.9182 - val_loss: 0.9462 - val_mean_squared_error: 0.9462\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9179 - mean_squared_error: 0.9179 - val_loss: 0.9471 - val_mean_squared_error: 0.9471\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9185 - mean_squared_error: 0.9185 - val_loss: 0.9441 - val_mean_squared_error: 0.9441\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9173 - mean_squared_error: 0.9173 - val_loss: 0.9431 - val_mean_squared_error: 0.9431\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9171 - mean_squared_error: 0.9171 - val_loss: 0.9439 - val_mean_squared_error: 0.9439\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9172 - mean_squared_error: 0.9172 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9171 - mean_squared_error: 0.9171 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914450642739685\n",
      "0.9437157484784983\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 1s 1ms/step - loss: 1.2564 - mean_squared_error: 1.2564 - val_loss: 1.3506 - val_mean_squared_error: 1.3506\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.2088 - mean_squared_error: 1.2088 - val_loss: 1.2934 - val_mean_squared_error: 1.2934\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 50us/step - loss: 1.1732 - mean_squared_error: 1.1732 - val_loss: 1.2459 - val_mean_squared_error: 1.2459\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 1.1464 - mean_squared_error: 1.1464 - val_loss: 1.2070 - val_mean_squared_error: 1.2070\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.1238 - mean_squared_error: 1.1238 - val_loss: 1.1748 - val_mean_squared_error: 1.1748\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.1043 - mean_squared_error: 1.1043 - val_loss: 1.1507 - val_mean_squared_error: 1.1507\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.0891 - mean_squared_error: 1.0891 - val_loss: 1.1290 - val_mean_squared_error: 1.1290\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0759 - mean_squared_error: 1.0759 - val_loss: 1.1113 - val_mean_squared_error: 1.1113\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0642 - mean_squared_error: 1.0642 - val_loss: 1.0953 - val_mean_squared_error: 1.0953\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0549 - mean_squared_error: 1.0549 - val_loss: 1.0814 - val_mean_squared_error: 1.0814\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 1.0446 - mean_squared_error: 1.0446 - val_loss: 1.0709 - val_mean_squared_error: 1.0709\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 1.0372 - mean_squared_error: 1.0372 - val_loss: 1.0593 - val_mean_squared_error: 1.0593\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0292 - mean_squared_error: 1.0292 - val_loss: 1.0508 - val_mean_squared_error: 1.0508\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 1.0414 - val_mean_squared_error: 1.0414\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.0329 - val_mean_squared_error: 1.0329\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 1.0259 - val_mean_squared_error: 1.0259\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 1.0019 - mean_squared_error: 1.0019 - val_loss: 1.0119 - val_mean_squared_error: 1.0119\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9978 - mean_squared_error: 0.9978 - val_loss: 1.0069 - val_mean_squared_error: 1.0069\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9939 - mean_squared_error: 0.9939 - val_loss: 1.0013 - val_mean_squared_error: 1.0013\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 55us/step - loss: 0.9902 - mean_squared_error: 0.9902 - val_loss: 0.9969 - val_mean_squared_error: 0.9969\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 0.9930 - val_mean_squared_error: 0.9930\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9831 - mean_squared_error: 0.9831 - val_loss: 0.9862 - val_mean_squared_error: 0.9862\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9799 - mean_squared_error: 0.9799 - val_loss: 0.9803 - val_mean_squared_error: 0.9803\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9771 - mean_squared_error: 0.9771 - val_loss: 0.9770 - val_mean_squared_error: 0.9770\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 65us/step - loss: 0.9743 - mean_squared_error: 0.9743 - val_loss: 0.9752 - val_mean_squared_error: 0.9752\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9712 - mean_squared_error: 0.9712 - val_loss: 0.9737 - val_mean_squared_error: 0.9737\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9684 - mean_squared_error: 0.9684 - val_loss: 0.9718 - val_mean_squared_error: 0.9718\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9658 - mean_squared_error: 0.9658 - val_loss: 0.9694 - val_mean_squared_error: 0.9694\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9637 - mean_squared_error: 0.9637 - val_loss: 0.9683 - val_mean_squared_error: 0.9683\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9617 - mean_squared_error: 0.9617 - val_loss: 0.9628 - val_mean_squared_error: 0.9628\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 0.9625 - val_mean_squared_error: 0.9625\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 0.9562 - mean_squared_error: 0.9562 - val_loss: 0.9620 - val_mean_squared_error: 0.9620\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 0.9552 - mean_squared_error: 0.9552 - val_loss: 0.9598 - val_mean_squared_error: 0.9598\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9524 - mean_squared_error: 0.9524 - val_loss: 0.9600 - val_mean_squared_error: 0.9600\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9506 - mean_squared_error: 0.9506 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9493 - mean_squared_error: 0.9493 - val_loss: 0.9565 - val_mean_squared_error: 0.9565\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 60us/step - loss: 0.9476 - mean_squared_error: 0.9476 - val_loss: 0.9565 - val_mean_squared_error: 0.9565\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9459 - mean_squared_error: 0.9459 - val_loss: 0.9538 - val_mean_squared_error: 0.9538\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 131us/step - loss: 0.9456 - mean_squared_error: 0.9456 - val_loss: 0.9505 - val_mean_squared_error: 0.9505\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9436 - mean_squared_error: 0.9436 - val_loss: 0.9473 - val_mean_squared_error: 0.9473\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9423 - mean_squared_error: 0.9423 - val_loss: 0.9491 - val_mean_squared_error: 0.9491\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9410 - mean_squared_error: 0.9410 - val_loss: 0.9472 - val_mean_squared_error: 0.9472\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9410 - mean_squared_error: 0.9410 - val_loss: 0.9447 - val_mean_squared_error: 0.9447\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 0.9456 - val_mean_squared_error: 0.9456\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 0.9468 - val_mean_squared_error: 0.9468\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9375 - mean_squared_error: 0.9375 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9361 - mean_squared_error: 0.9361 - val_loss: 0.9421 - val_mean_squared_error: 0.9421\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 0.9358 - mean_squared_error: 0.9358 - val_loss: 0.9411 - val_mean_squared_error: 0.9411\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 91us/step - loss: 0.9346 - mean_squared_error: 0.9346 - val_loss: 0.9411 - val_mean_squared_error: 0.9411\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9347 - mean_squared_error: 0.9347 - val_loss: 0.9422 - val_mean_squared_error: 0.9422\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9334 - mean_squared_error: 0.9334 - val_loss: 0.9408 - val_mean_squared_error: 0.9408\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9327 - mean_squared_error: 0.9327 - val_loss: 0.9394 - val_mean_squared_error: 0.9394\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9322 - mean_squared_error: 0.9322 - val_loss: 0.9404 - val_mean_squared_error: 0.9404\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9333 - mean_squared_error: 0.9333 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9310 - mean_squared_error: 0.9310 - val_loss: 0.9408 - val_mean_squared_error: 0.9408\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9310 - mean_squared_error: 0.9310 - val_loss: 0.9383 - val_mean_squared_error: 0.9383\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9297 - mean_squared_error: 0.9297 - val_loss: 0.9408 - val_mean_squared_error: 0.9408\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9291 - mean_squared_error: 0.9291 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9287 - mean_squared_error: 0.9287 - val_loss: 0.9414 - val_mean_squared_error: 0.9414\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9280 - mean_squared_error: 0.9280 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9276 - mean_squared_error: 0.9276 - val_loss: 0.9416 - val_mean_squared_error: 0.9416\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 0.9394 - val_mean_squared_error: 0.9394\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9265 - mean_squared_error: 0.9265 - val_loss: 0.9393 - val_mean_squared_error: 0.9393\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9254 - mean_squared_error: 0.9254 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9261 - mean_squared_error: 0.9261 - val_loss: 0.9437 - val_mean_squared_error: 0.9437\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 0.9436 - val_mean_squared_error: 0.9436\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9258 - mean_squared_error: 0.9258 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9239 - mean_squared_error: 0.9239 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9238 - mean_squared_error: 0.9238 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9230 - mean_squared_error: 0.9230 - val_loss: 0.9421 - val_mean_squared_error: 0.9421\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 0.9223 - mean_squared_error: 0.9223 - val_loss: 0.9420 - val_mean_squared_error: 0.9420\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 101us/step - loss: 0.9216 - mean_squared_error: 0.9216 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 111us/step - loss: 0.9220 - mean_squared_error: 0.9220 - val_loss: 0.9370 - val_mean_squared_error: 0.9370\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 95us/step - loss: 0.9206 - mean_squared_error: 0.9206 - val_loss: 0.9389 - val_mean_squared_error: 0.9389\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9207 - mean_squared_error: 0.9207 - val_loss: 0.9418 - val_mean_squared_error: 0.9418\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 98us/step - loss: 0.9205 - mean_squared_error: 0.9205 - val_loss: 0.9395 - val_mean_squared_error: 0.9395\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9197 - mean_squared_error: 0.9197 - val_loss: 0.9410 - val_mean_squared_error: 0.9410\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9191 - mean_squared_error: 0.9191 - val_loss: 0.9412 - val_mean_squared_error: 0.9412\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9192 - mean_squared_error: 0.9192 - val_loss: 0.9425 - val_mean_squared_error: 0.9425\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9186 - mean_squared_error: 0.9186 - val_loss: 0.9400 - val_mean_squared_error: 0.9400\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9180 - mean_squared_error: 0.9180 - val_loss: 0.9419 - val_mean_squared_error: 0.9419\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9186 - mean_squared_error: 0.9186 - val_loss: 0.9404 - val_mean_squared_error: 0.9404\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9186 - mean_squared_error: 0.9186 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9164 - mean_squared_error: 0.9164 - val_loss: 0.9390 - val_mean_squared_error: 0.9390\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9171 - mean_squared_error: 0.9171 - val_loss: 0.9392 - val_mean_squared_error: 0.9392\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9166 - mean_squared_error: 0.9166 - val_loss: 0.9388 - val_mean_squared_error: 0.9388\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 0.9363 - val_mean_squared_error: 0.9363\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9160 - mean_squared_error: 0.9160 - val_loss: 0.9380 - val_mean_squared_error: 0.9380\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 88us/step - loss: 0.9157 - mean_squared_error: 0.9157 - val_loss: 0.9397 - val_mean_squared_error: 0.9397\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 0.9415 - val_mean_squared_error: 0.9415\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 0.9416 - val_mean_squared_error: 0.9416\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 93us/step - loss: 0.9141 - mean_squared_error: 0.9141 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9136 - mean_squared_error: 0.9136 - val_loss: 0.9429 - val_mean_squared_error: 0.9429\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9140 - mean_squared_error: 0.9140 - val_loss: 0.9426 - val_mean_squared_error: 0.9426\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9132 - mean_squared_error: 0.9132 - val_loss: 0.9433 - val_mean_squared_error: 0.9433\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9134 - mean_squared_error: 0.9134 - val_loss: 0.9430 - val_mean_squared_error: 0.9430\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9135 - mean_squared_error: 0.9135 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 99/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9127 - mean_squared_error: 0.9127 - val_loss: 0.9429 - val_mean_squared_error: 0.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9122 - mean_squared_error: 0.9122 - val_loss: 0.9440 - val_mean_squared_error: 0.9440\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9114701248328614\n",
      "0.9440459674134032\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay with Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= sgd ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8383352017382003\n",
      "0.9126765851437523\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb  \n",
    "\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database  \n",
    "\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/  \n",
    "\n",
    "* https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, we began to practice some of the concepts regarding normalization and optimization for neural networks. In the final lab for this section, you'll independently practice these concepts on your own in order to tune a model to predict individuals payments to loans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
